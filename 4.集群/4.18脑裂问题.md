# Split Brain Resolver

在操作Akka集群时，必须考虑如何处理网络分区(又称裂脑场景)和机器崩溃(包括JVM和硬件故障)。如果您使用集群单例或集群分片，尤其是与Akka Persistence一起使用，这对于正确的行为至关重要。

"裂脑解析器"视频是一个很好的起点：学习为什么使用正确的downing提供程序很重要，以及裂脑解析器如何工作的。

# 模块信息
Akka Split Brain Resolver是akka-cluster的一部分，您可能已经包含了该依赖项。否则，在您的项目中添加以下依赖项：
```conf
```
# 启用裂脑解析器
您需要通过在ActorSystem(application.conf)的配置中将其配置为Downing提供程序来启用Split Brain Resolver：
```conf
```
您还应该考虑其他可用的下降策略。

# 问题
对于观察者而言，分布式系统中的一个基本问题是，网络分区(裂脑场景)和机器崩溃是无法区分的，一个节点可以观察到另一个节点存在问题，但是它无法确定它是否已崩溃并且永远不会再次可用，或者由于网络问题可能会或可能不会在一段时间后再次治愈。暂时性和永久性故障是无法区分的，因为必须在有限的时间内做出决定，并且总是存在持续时间长于决策时间限制的暂时性故障。

第三类问题是过程是否没有响应，例如由于过载，CPU饥饿或长时间的垃圾回收暂停。这与网络分区和崩溃也没有区别。我们唯一需要做出决定的信号是"在给定的时间内没有对心跳的回复"，这意味着造成延迟或心跳丢失的现象是彼此无法区分的，必须以相同的方式进行处理。

发生崩溃时，我们希望立即从集群成员关系中删除受影响的节点。当存在网络分区或无响应的过程时，我们希望等待一会儿，希望它是一个暂时的问题，该问题将再次得到解决，但是在某些时候，我们必须放弃并继续使用网络分区一侧的节点，并关闭另一端的节点。此外，某些功能在分区期间并不完全可用，因此，如果分区花费的时间太长，则该分区是否瞬态不重要。这两个目标相互冲突，并且我们可以在删除崩溃节点的时间，与对瞬态网络分区进行过早的操作之间进行权衡。

鉴于网络分区不同侧的节点无法通信，这是一个很难解决的问题。我们必须确保双方可以自己做出决定，并且就哪一部分将继续运行以及哪一部分将自行关闭做出相同的决定。

另一种问题很难看清"正确"的图景，即某些节点未完全连接且无法直接相互通信，但可以通过其他节点在它们之间传播信息。

Akka集群具有故障检测器，该故障检测器会注意到网络分区和机器崩溃(但无法区分两者)。它使用定期的心跳消息来检查其他节点是否可用并且运行状况良好。故障检测器的这些观察结果称为节点不可达，如果故障检测器观察到它可以再次通信，则该节点可能再次变为可达。

故障检测器本身不足以在所有情况下做出正确的决定。简单的方法是在超时后从集群成员中删除无法访问的节点。这对于崩溃和短暂的瞬态网络分区非常有用，但不适用于较长的网络分区。网络分区的两侧将看到另一端无法访问，不久后将其从其集群成员中删除。由于这种情况在双方都发生，因此结果是创建了两个单独的断开连接的集群。此方法由OSS版本的Akka Cluster中的启用(默认情况下为禁用)功能提供。

如果将基于超时的自动关闭功能与"集群单例"或"集群分片"结合使用，则意味着将运行两个具有相同标识符的单例实例或两个分片实体。一个会在运行：每个集群一个。例如，当与Akka Persistence一起使用时，可能导致具有相同persistenceId的持久性actor的两个实例正在运行并同时写入同一持久性事件流，这在重播这些事件时将具有致命的后果。

Akka Cluster中的默认设置是不自动删除无法访问的节点，建议您由操作员或外部监视系统来决定要做什么。这是一个有效的解决方案，但是如果您由于其他原因而没有此人员或外部系统，则不是很方便。

如果根本无法关闭不可达节点，则它们仍将成为集群成员的一部分。这意味着"集群单例"和"集群分片"将不会故障转移到另一个节点。当存在无法访问的节点时，不会把加入集群的新节点提升为完全有价值的成员(状态为Up)。同样，在解决所有无法访问的节点之前，不会删除离开成员。换句话说，将无法访问的成员保留无限时间是不希望的。

有了问题域的介绍，就该看看提供的用于处理网络分区，无响应节点和崩溃节点的策略了。

# 策略
默认情况下，将使用"保留多数"策略，因为该策略在大多数系统上都适用。但是，考虑其他可用策略并选择适合您系统特征的策略会很麻烦。例如，在Kubernetes环境中，租赁策略可能是一个不错的选择。

每个策略都有一个失败场景，在该场景中会做出"错误"的决定。本节介绍何时使用什么策略和准则。

当存在不确定性时，它将选择关闭比必要数量更多的节点，甚至关闭所有节点。因此，应该始终将Split Brain Resolver与一种机制结合使用，以自动启动已关闭的节点，并将它们加入现有集群或再次形成新集群。

您可以使用配置属性akka.cluster.split-brain-resolver.active-strategy启用策略。

## 稳定之后
在集群成员关系和有关不可达节点的信息在特定时间段内保持稳定之前，所有策略均处于非活动状态。存在网络分区的情况下连续添加更多节点不会影响此超时，因为在存在无法访问的节点时，这些节点的状态不会更改为Up。策略逻辑中不计入连接节点。

```conf
```

将akka.cluster.split-brain-resolver.stable-after设置为较短的持续时间，以更快地删除崩溃的节点，但这样做的代价是对过渡网络分区采取过早的处理措施，否则可能会恢复原状。 请勿将其设置为比集群中的成员传播时间短的持续时间，后者取决于集群的大小。 不同集群大小的建议最小持续时间：
```conf
```

不同的策略可能具有以下描述的其他设置。

>注意
在所有节点上使用相同的配置很重要。

决定关闭自身的分裂一侧，使用cluster down命令启动对集群成员的移除。将其散布在可到达的节点之间后，它将从集群成员中删除。

从集群中删除节点后，最好终止ActorSystem并退出JVM。

这是由Coordinated Shutdown 处理的，但是要退出JVM，建议您启用以下操作：

```conf
```
>注意
一些旧容器可能会阻止对System.exit(..)的调用，您可能必须找到其他方法来关闭应用程序。例如，在Spring/Tomcat设置之上运行Akka时，可以将对System.exit(..)的调用替换为对Spring的ApplicationContext.close()方法的调用(或对Tomcat Manager的API的HTTP调用取消部署该应用程序)。

## 保持多数
如果基于最新已知的成员信息，当前节点占大多数，则keep-majority策略将关闭不可达节点。否则沿着可到达的节点，即自己的部分。如果部分大小相等，则将保留包含地址最低的节点的部分。

当集群中的节点数动态更改并且因此无法使用静态仲裁时，此策略是一个不错的选择。

此策略还处理在发生网络分区的同时发生成员身份更改，可能发生的边缘情况。例如，两个成员的状态在一侧更改为"启动"，但是在断开连接之前该信息不会传播到另一侧。然后，一侧会看到另外两个节点，并且双方可能会认为自己拥有多数。如果将加入节点的另一侧更改为" Up"，它将检测到这种情况并做出安全的决定来关闭该侧上所有少数节点。请注意，这样做的缺点是，如果未将连接节点更改为Up并在另一侧成为多数，则每个部分都将自行关闭，从而终止整个集群。

请注意，如果有两个以上的分区，但大多数分区都不占多数，则每个部分都会自行关闭，从而终止整个集群。

如果有一半以上的节点同时崩溃，则其他正在运行的节点将自行关闭，因为它们认为自己不是多数节点，从而终止了整个集群。

该决定可以基于具有配置角色的节点，而不是集群中的所有节点。当某些类型的节点比其他类型更有价值时，这可能会很有用。例如，您可能有一些负责持久性数据的节点和一些具有无状态工作程序服务的节点。然后，即使意味着要关闭更多的工作程序节点，保留尽可能多的持久数据节点可能更为重要。

# 配置
```conf
```

## 静态仲裁
如果剩余节点数大于或等于配置的仲裁大小，则名为静态仲裁的策略将关闭不可达节点。否则，它将关闭可访问的节点，即它将关闭分区的另一侧。换句话说，仲裁大小定义了集群必须必须运行的最小节点数。

当集群中有固定数量的节点，或者可以定义具有特定角色的固定数量的节点时，此策略是一个不错的选择。

例如，在9个节点的集群中，您将仲裁大小配置为5。如果网络拆分为4个和5个节点，则5个节点的一侧将继续存在，而其他4个节点将被关闭。此后，在5节点集群中，无法处理更多故障，因为剩余集群大小将小于5。在该5节点集群中发生另一个故障的情况下，所有节点都将被关闭。

因此，在删除旧节点后加入新节点非常重要。

这样做的另一个结果是，如果启动集群时存在无法访问的节点，则在达到此限制之前，集群可能会立即关闭自身。如果您在大约相同的时间启动所有节点，或在leader将"Joining"成员的成员状态更改为"UP"之前，使用akka.cluster.min-nr-of-members定义所需的成员数，则这不是问题。您可以在做出downing决策后，使用stable-after设置，调节超时时间。

添加到集群中的成员数量不应超过quorum-size*2-1。如果违反此建议，则会记录一条警告。如果在需要SBR决策时仍保留超出的集群大小，则它将关闭所有节点，因为可能会导致双方彼此中断从而形成两个单独的集群的风险。

对于滚动更新，最好通过协调关机(SIGTERM)正常退出集群。为了成功离开，将不使用SBR(no downing)，但是如果在滚动更新进行的同时存在无法访问性问题，则可以做出SBR决定。为避免滚动更新期间不超过成员总数限制，建议在使用静态仲裁时离开并完全删除一个节点，然后再添加新节点。

如果将集群分为3个(或更多)部分，则每个部分都小于配置的仲裁大小，这将导致自身崩溃并可能关闭整个集群。

如果同时出现的节点数量多于配置的仲裁大小崩溃，则其他正在运行的节点将自行关闭，因为它们认为自己不在多数节点中，从而终止了整个集群。

该决定可以基于具有配置角色的节点，而不是集群中的所有节点。当某些类型的节点比其他类型更有价值时，这可能会很有用。例如，您可能有一些负责持久性数据的节点和一些具有无状态工作程序服务的节点。然后，即使意味着要关闭更多的工作程序节点，保留尽可能多的持久数据节点可能更为重要。

角色还有另一个用途。把集群中的几个(例如7个)稳定节点定义角色，并在静态仲裁配置中使用该角色，您将能够在没有该角色的情况下动态添加和删除其他节点，并且在网络分区的情况，仍然可以很好地决定保留哪些节点以及关闭哪些节点。与保持多数状态(如下所述)相比，此方法的优势在于您无需承担将集群分成两个单独的集群的风险，即脑裂*。您仍然必须遵守以下规则：不要使用此角色启动太多节点。如上所述，如果在集群中没有足够的角色的节点时发生故障，它也有关闭所有节点的风险。

配置：
```conf
```
```conf
```

## 保持最旧
keep-oldest的策略将删除不包含最旧成员的部分。最旧的成员很有趣，因为活动的Cluster Singleton实例在最旧的成员上运行。

如果将down-if-alone配置为on，则此规则有一个例外。然后，如果最旧的节点已与所有其他节点进行了分区，则最旧的节点将自行关闭并保持所有其他节点运行。当集群中剩余唯一的节点时，该策略将不会关闭该节点。

请注意，如果最旧的节点崩溃，当down-if-alone是on，则其他节点将其从集群中删除，否则，如果最旧的节点崩溃，它们将自行关闭，即关闭整个集群以及最旧的节点。

如果您使用集群单例，并且不想关闭运行单例实例的节点，则可以使用此策略。如果最旧的节点崩溃，则新的单例实例将在下一个最旧的节点上启动。缺点是该策略可能在大型集群中仅保留几个节点。例如，如果最早的一部分包含2个节点，而另一部分包含98个节点，则它将保留2个节点并关闭98个节点。

此策略还处理边缘情况，在发生网络分区的同时发生成员身份更改。例如，最早的成员的状态更改为一侧退出，但在断开连接之前该信息不会传播到另一侧。它将检测到这种情况，并做出安全的决定，将最旧的那一侧视为"离开"。请注意，这样做的缺点是，如果最旧的是"离开"并且未更改为"退出"，则每个部分都会自行关闭，从而终止整个集群。

该决定可以基于具有已配置角色的节点而不是集群中的所有节点，即，使用具有该角色的节点中最旧的成员(单个)。

配置
```conf
```
```conf
```

## 全部down
down-all策略将关闭所有节点。

如果网络环境高度不稳定且无法完全观察到无法访问的观察结果，并且包括频繁发生的间接连接节点，则此策略可以是一种安全的选择。由于不稳定，在分区的不同侧上增加不同信息的风险，因此其他策略可能会导致决策冲突。在这种环境中，最好关闭所有节点并启动新的新集群。

关闭所有节点意味着在节点重新启动并形成新集群之前，系统将完全不可用。不建议对大型集群(> 10个节点)使用此策略，因为任何较小的问题都会关闭所有节点，并且在较大的集群中更可能发生这种情况，因为有更多的节点可能会发生故障。

另请参阅在不稳定且间接连接的节点时全部关闭。

## 租约
"多数租约"的策略正在使用分布式租约(锁定)来决定允许哪些节点存活。只有一个SBR实例可以获取租约，以决定保留该租约。另一方将无法获得租约，因此将自行撤消。

尽最大的努力保留具有最多节点的一方，即多数一方。这是通过在尝试获得少数派一方的租约之前增加延迟来实现的。

当前有一种受支持的租约实现，由Kubernetes中的自定义资源定义(CRD)支持。 Kubernetes租赁文档中对此进行了描述。

该策略非常安全，因为协调是由外部仲裁员添加的。与其他策略相比，需要进行权衡的是，它需要用于实施租赁的其他基础结构，并且将决策的可用性降低到支持租赁store的系统的可用性。

与其他策略类似，请勿延迟决策，这一点很重要，因为无法获取租约的节点必须决定自行关闭，如果不稳定，请参阅全部关闭。

在某些情况下，当需要所有SBR实例做出决定时，例如，因为它位于网络分区的另一侧，因此所有节点都将被关闭。

## 组态：
```conf
```
```conf
```
另请参阅Kubernetes租赁中的配置和其他依赖关系

# 间接连接的节点
在发生故障的网络中，有时会发现某些节点无法通过某些网络链接访问节点，但它们仍通过其他节点间接连接，即不是干净的网络分区(或节点崩溃)。

当检测到这种情况时，"裂脑解析器"将保持完全连接的节点, 并使所有间接连接的节点保持down状态。

如果存在间接连接的节点和干净的网络分区的组合，则它将上述决策与普通决策结合起来，例如在排除可疑故障检测观察结果之后，保持多数。

# 不稳定时全部down
当故障检测器的可达性观测值发生变化时，SBR决策将推迟直到stable-after时间内没有变化。如果持续的时间过长，则可能表明系统/网络不稳定，并且可能导致网络分区的各个方面的决策延迟或冲突。

作为该情况的预防措施，如果在第一个不可访问事件之后的table-after + down-all-when-unstable没有做出决定，则所有节点都将关闭。如果所有无法到达的地方都已被治愈，downed或去除，或者在stable-after * 2内没有变化，则重置测量。

默认情况下，所有策略均启用此功能，默认情况下，持续时间推导为stable-after的3/4。

下面的属性可以定义为stable-after后可以接受继续更改的持续时间，也可以将其设置为off以禁用此功能。
```conf```
>警告
建议保持所有不稳定状态均保持启用状态，并且不要将其设置为比之后稳定时间(向下移除余量)更长的持续时间，因为这可能会导致本应降低的方面的决策延迟，例如在干净的网络分区的情况下，那一侧的持续不稳定应该关闭。这可能导致成员从一侧移开，但仍在另一侧运行。

# 多个数据中心
Akka Cluster支持多个数据中心，其中集群成员资格由每个数据中心分别管理，并且独立于不同数据中心的网络分区。 Split Brain Resolver正在采用该策略，并且不会计算另一个数据中心中的节点或down的节点。

当跨数据中心的网络分区时，典型的解决方案是等待分区恢复正常，即不执行任何操作。其他决定应由外部监控工具或人工操作员执行。

# 集群单例和集群分片
集群单例和集群分片的目的是在任何时间点最多运行给定actor的一个实例。当关闭此类实例时，应该在集群中的其他位置启动一个新实例。重要的是，在停止旧实例之前，不要启动新实例。当单例或分片实例是持久性实例时，这尤其重要，因为持久性actor实例的日志事件必须只有一个活动writer。

由于网络分区不同侧的策略无法相互通信，并且它们可能在稍有不同的时间点做出决策，因此必须有基于时间的余量，以确保在停止旧实例之前不启动新实例。

您希望将其配置为较短的时间以进行快速故障转移，但是这会增加同时运行多个单例/分片实例的风险，并且可能需要花费不同的时间来执行决策(down/移除)。默认情况下，持续时间与stable-after属性相同(请参阅上文稳定)。建议将此值保持不变，但也可以使用akka.cluster.down-removal-margin属性单独覆盖它。

设置此after-after/akka.cluster.down-removal-margin的另一个问题是处理JVM暂停，例如垃圾收集。当节点无响应时，不知道它是由于暂停，过载，崩溃还是网络分区引起的。如果暂停的持续时间长于stable-after*2，则SBR需要时间来使节点down，并让单例和分片在其他节点上启动。当节点取消暂停时，将有很短的时间才能看到其自身出现故障，此时单例对象和分片actor仍在运行。因此，重要的是要了解您的应用程序可能产生的最大暂停时间，并确保它小于stable-margin。

如果选择为down-removal-margin设置一个单独的值，则建议的不同集群大小的最小持续时间为：
```table
```

## 预期的故障转移时间
如您所见，有几个配置的超时会增加总故障转移延迟。 使用默认配置，这些是：

- 故障检测5秒
- stable-after 20
- down-removal-margin(默认与stable-after相同)20秒

总体而言，使用默认配置，您可以预期单例或分片实例的故障转移时间约为45秒。 默认配置的大小为100个节点的集群的大小。 如果您有大约10个节点，则可以将stable-after减少到大约10秒，从而导致预期的故障转移时间大约为25秒。