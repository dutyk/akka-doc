# 依赖
要使用持久性查询，必须在项目中添加以下依赖项：
```xml
<properties>
 <akka.version>2.6.8</akka.version> <scala.binary.version>2.13</scala.binary.version></properties>
<dependency>
 <groupId>com.typesafe.akka</groupId> <artifactId>akka-persistence-query_${scala.binary.version}</artifactId> <version>${akka.version}</version></dependency>
```
这还要增加对Akka Persistence模块的依赖。
# 介绍
Akka持久性查询通过提供基于通用异步流的查询接口来补充事件溯源，各种日志插件可以实现该接口以暴露其查询功能。
持久性查询的最典型用例是在流行的CQRS架构模式中实现所谓的查询侧(也称为"读取侧")，其中应用程序的写侧(例如，使用Akka持久性实现)是从"查询端"完全分离的。Akka Persistence Query本身并不直接是应用程序的查询方，但是它可以帮助将数据从写入方迁移到查询方数据库。在非常简单的情况下，Persistence Query可能足以满足您的应用程序的查询需求，但是我们强烈建议(本着CQRS的精神)根据需要将写/读端拆分为单独的数据存储。
带有Akka 2.6视频的CQRS是学习如何使用eventsByTag来实现带有Akka的CQRS的一个很好的起点。另外，请观看Akka 2.6视频的事件溯源介绍。
# 设计概述
Akka持久性查询专门设计为非常松散地特定的API。这是为了使所提供的API具有足够的通用性，以使每个日志实现都能够展示其最佳功能，例如SQL日志可以使用复杂的SQL查询，或者如果日志能够订阅实时事件流，则还应该可以暴露相同的API-类型化的事件流。
每个读日志都必须明确说明它支持的查询类型。有关其支持的查询和语义的详细信息，请参阅日志的插件文档。
尽管Akka Persistence Query并未提供ReadJournals的实际实现，但它为大多数常见的查询场景定义了许多预定义的查询类型，大多数日志都可能实现(但并非必须如此)。
# 读日志
为了发出查询，必须首先获得ReadJournal的实例。已读日志是作为社区插件实现的，每个都针对特定的数据存储(例如Cassandra或JDBC数据库)。例如，给定一个提供akka.persistence.query.my-read-journal的库，获取相关日志就很简单：
```java
// obtain read journal by plugin id
final MyJavadslReadJournal readJournal =
 PersistenceQuery.get(system) .getReadJournalFor( MyJavadslReadJournal.class, "akka.persistence.query.my-read-journal");
// issue query to journal
Source<EventEnvelope, NotUsed> source =
 readJournal.eventsByPersistenceId("user-1337", 0, Long.MAX_VALUE);
// materialize stream, consuming events
source.runForeach(event -> System.out.println("Event: " + event), system);
```
鼓励日志实现将此标识符放入用户已知的变量中，以便可以通过getJournalFor(NoopJournal.class，NoopJournal.identifier)访问它，但是不强制这样做。
读日志的实现可作为社区插件获得。
## 预定义查询
Akka持久性查询带有许多内置的查询接口，并建议Journal实施者根据下面描述的语义来实现它们。重要的是要注意，尽管这些查询类型非常普遍，但日志并非必须实现所有查询类型，例如，因为在给定的日志中，此类查询的效率非常低。
>注意
有关所支持的查询类型的特定列表，请参考ReadJournal插件的文档。例如，日志插件应记录其流完成策略。
预定义的查询是：
### PersistenceIdsQuery和CurrentPersistenceIdsQuery
persistenceIds，旨在允许用户订阅系统中所有持久性ID的流。默认情况下，应将此流假定为"实时"流，这意味着日志在进入系统时应继续发出新的持久性ID：
```java
readJournal.persistenceIds();
```
如果您的用法不需要实时流，则可以使用currentPersistenceIds查询：
```java
readJournal.currentPersistenceIds();
```
### EventsByPersistenceIdQuery和CurrentEventsByPersistenceIdQuery
eventsByPersistenceId是等同于重播事件源actor的查询，但是，由于它是一个流，因此可以将其保持活动状态,由给定persistenceId标识的持久性actor监控它持久化的其他传入事件。
```java
readJournal.eventsByPersistenceId("user-us-1337"，0L，Long.MAX_VALUE);
```
为了实现此目的，大多数日志将不得不恢复为轮询，通常可以使用refresh-interval配置属性对其进行配置。
如果您的用法不需要实时流，则可以使用currentEventsByPersistenceId查询。
### EventsByTag和CurrentEventsByTag
eventsByTag允许查询事件，无论它们与哪个persistenceId相关联。该查询很难在某些日志中实施，或者可能需要对使用的数据存储进行一些额外的准备才能有效执行。该查询的目的是允许查询所有带有特定标记"标记"的事件。这包括用例来查询聚合根类型的所有域事件。请参阅您已阅读的日志插件的文档，以了解是否以及如何对其进行支持。
某些日志可能支持事件标记或将这些事件包装在带有给定标记的akka.persistence.journal.Tagged中的事件适配器。日志可能支持其他进行标记的方法-同样，实现的确切程度取决于所使用的日志。这是一个使用EventSourcedBehavior进行标记的示例：
```java
private final String entityId;
public static final int NUMBER_OF_ENTITY_GROUPS = 10;
@Override
public Set<String> tagsFor(Event event) {
 String entityGroup = "group-" + Math.abs(entityId.hashCode() % NUMBER_OF_ENTITY_GROUPS); Set<String> tags = new HashSet<>(); tags.add(entityGroup); if (event instanceof OrderCompleted) tags.add("order-completed"); return tags;}
```
>注意
使用跨越多个persistenceIds的查询(例如EventsByTag)时要记住的一个非常重要的事情是，事件出现在流中的事件顺序很少保证(或在materializations之间保持稳定)。
日志可以选择对事件进行严格排序，然后应明确记录其提供的排序保证-例如，在关系数据库上很容易实现"例如，按时间戳升序排序，与persistenceId无关"，这很容易实现，但可能很难在普通键值数据存储区上有效实施。
在下面的示例中，我们查询所有已标记的事件(我们假设这是由写端使用事件或事件适配器标记执行的，或者日志足够聪明以至于可以弄清楚我们用这个标记表示什么- 例如，如果日志将事件存储为json，则它可能会尝试查找将field标签设置为该值的事件，等等)。
```java
// assuming journal is able to work with numeric offsets we can:
final Source<EventEnvelope, NotUsed> completedOrders =
 readJournal.eventsByTag("order-completed", new Sequence(0L));
// find first 10 completed orders:
final CompletionStage<List<OrderCompleted>> firstCompleted =
 completedOrders .map(EventEnvelope::event) .collectType(OrderCompleted.class) .take(10) // cancels the query stream after pulling 10 elements .runFold( new ArrayList<>(10), (acc, e) -> { acc.add(e); return acc; }, system);
// start another query, from the known offset
Source<EventEnvelope, NotUsed> furtherOrders =
 readJournal.eventsByTag("order-completed", new Sequence(10));```
如您所见，我们可以在结果查询流上使用Streams可用的所有常用流运算符，例如，获取前10个并取消该流。值得指出的是，内置的EventsByTag查询具有可选支持的offset参数(Long类型)，日志可以使用该参数实现可恢复流。例如，日志可能能够使用WHERE子句从特定行开始读取，或者在能够通过插入时间对事件进行排序的数据存储中，它可以将Long视为时间戳记并仅选择较旧的事件。
如果您的用法不需要实时流，则可以使用currentEventsByTag查询。
## 查询的Materialized值
日志能够通过暴露Materialized值来提供与查询有关的其他信息，这是Streams的一项功能，它允许在Materialized 时公开其他值。
更高级的查询日志可能会使用此技术来公开有关实例化流的字符的信息，例如，如果它是有限的或无限的，严格排序的或根本没有排序的。Materialized值类型定义为返回的Source的第二个类型参数，它允许日志为用户提供其专门的查询对象，如以下示例所示：
```java
static class RichEvent {
 public final Set<String> tags; public final Object payload;
 public RichEvent(Set<String> tags, Object payload) { this.tags = tags; this.payload = payload; }}
// a plugin can provide:
static final class QueryMetadata {
 public final boolean deterministicOrder; public final boolean infinite;
 public QueryMetadata(boolean deterministicOrder, boolean infinite) { this.deterministicOrder = deterministicOrder; this.infinite = infinite; }}
```
```java
public Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) {
```
```java
Set<String> tags = new HashSet<String>();
tags.add("red");
tags.add("blue");
final Source<RichEvent, QueryMetadata> events =
 readJournal .byTagsWithMeta(tags) .mapMaterializedValue( meta -> { System.out.println( "The query is: " + "ordered deterministically: " + meta.deterministicOrder + " " + "infinite: " + meta.infinite); return meta; });
events
 .map( event -> { System.out.println("Event payload: " + event.payload); return event.payload; }) .runWith(Sink.ignore(), system);```
# 性能和非规范化
使用事件来源和CQRS([命令和查询责任隔离](https://docs.microsoft.com/zh-cn/previous-versions/msp-np/jj554200(v = pandp.10))?redirectedfrom=MSDN))技术，认识到写端与读端的需求完全不同是极其重要的，将这些关注点分离到针对任一端进行了优化的数据存储中，就可以同时为写和读提供最佳体验双方。
例如，在投标系统中，重要的是"接受报价"并尽快响应竞标人我们已经接受了报价，这意味着写吞吐量对于写方至关重要。意味着能够扩展以适应这些需求的数据存储在查询时具有较少表达。
另一方面，同一应用程序可能具有一些复杂的统计数据视图，或者我们可能需要分析师使用数据来找出最佳的出价策略和趋势-这通常需要某种表达性查询功能，例如SQL或编写Spark作业进行分析数据。因此，需要将写侧中存储的数据投影到其他经过读取优化的数据存储中。
>注意
在Akka Persistence中引用Materialized视图时，应将其视为"查询结果的某种持久存储"。换句话说，这意味着视图被创建一次，以便随后被多次查询，因为采用这种格式，查询视图可能更有效或更有趣(而不是直接源事件)。
## 将视图具体化为与Reactive Streams兼容的数据存储
如果读取的数据存储公开了Reactive Streams接口，则实现简单的投影就像使用read-journal并将其馈送到数据库驱动程序接口一样简单，例如：
```java
final ReactiveStreamsCompatibleDBDriver driver = new ReactiveStreamsCompatibleDBDriver();
final Subscriber<List<Object>> dbBatchWriter = driver.batchWriter();
// Using an example (Reactive Streams) Database driver
readJournal
 .eventsByPersistenceId("user-1337", 0L, Long.MAX_VALUE) .map(envelope -> envelope.event()) .grouped(20) // batch inserts into groups of 20 .runWith(Sink.fromSubscriber(dbBatchWriter), system); // write batches to read-side database```
## 使用mapAsync实现视图
如果目标数据库未提供可以执行写操作的反应流订阅服务器，则可能必须使用普通函数或Actor来实现写逻辑。
如果您的写入逻辑是无状态的，并且您需要在写入替代数据存储之前将事件从一种数据类型转换为另一种数据类型，则投影将如下所示：
```java
static class ExampleStore {
 CompletionStage<Void> save(Object any) { // ... }}
```
```java
final ExampleStore store = new ExampleStore();
readJournal
 .eventsByTag("bid", new Sequence(0L)) .mapAsync(1, store::save) .runWith(Sink.ignore(), system);```
## 可恢复的预测
有时，您可能需要实现"可恢复"的投影，而这些投影并非每次运行时都从时间的起点开始。 在这种情况下，您将需要存储已处理事件的序列号(或偏移量)，并在下次启动此投影时使用它。 此模式不是内置的，但是实现起来很简单。
下面的示例还突出显示了如何使用Actors来实现写端，以防万一您需要执行一些复杂的逻辑，而该逻辑最好在Actor内部进行处理，然后再将事件持久化到另一个数据存储中：
```java
 final Duration timeout = Duration.ofSeconds(3);
 final MyResumableProjection bidProjection = new MyResumableProjection("bid");
 long startFromOffset = bidProjection.latestOffset().toCompletableFuture().get(3, TimeUnit.SECONDS);
 readJournal .eventsByTag("bid", new Sequence(startFromOffset)) .mapAsync( 8, envelope -> { final CompletionStage<Done> f = AskPattern.ask( writer, (ActorRef<Done> replyTo) -> new TheOneWhoWritesToQueryJournal.Update(envelope.event(), replyTo), timeout, system.scheduler()); return f.thenApplyAsync(in -> envelope.offset(), system.executionContext()); }) .mapAsync(1, offset -> bidProjection.saveProgress(offset)) .runWith(Sink.ignore(), system);}
```
```java
static final class TheOneWhoWritesToQueryJournal
 extends AbstractBehavior<TheOneWhoWritesToQueryJournal.Command> {
 interface Command {}
 static class Update implements Command { public final Object payload; public final ActorRef<Done> replyTo;
 Update(Object payload, ActorRef<Done> replyTo) { this.payload = payload; this.replyTo = replyTo; } }
 public static Behavior<Command> create(String id, ExampleStore store) { return Behaviors.setup(context -> new TheOneWhoWritesToQueryJournal(context, store)); }
 private final ExampleStore store;
 private ComplexState state = new ComplexState();
 private TheOneWhoWritesToQueryJournal(ActorContext<Command> context, ExampleStore store) { super(context); this.store = store; }
 @Override public Receive<Command> createReceive() { return newReceiveBuilder().onMessage(Update.class, this::onUpdate).build(); }
 private Behavior<Command> onUpdate(Update msg) { state = updateState(state, msg); if (state.readyToSave()) store.save(Record.of(state)); return this; }
 ComplexState updateState(ComplexState state, Update msg) { // some complicated aggregation logic here ... return state; }}
```
# 查询插件
查询插件适用于各种可用数据存储区，有多种(主要是社区驱动的)ReadJournal实现。可用插件的完整列表在Akka Persistence Query Community插件页面上维护。
LevelDB的持久性查询中介绍了LevelDB的插件。
本部分旨在提供技巧和指南，以帮助插件开发人员实现自定义查询插件。除非针对尚未受支持的数据存储，否则大多数用户将不需要自己实施日志。
>注意
由于不同的数据存储提供不同的查询功能，因此日志插件必须广泛地记录其公开的语义以及处理的查询方案。
## ReadJournal插件API
读取日志插件必须实现akka.persistence.query.ReadJournalProvider，该实例创建akka.persistence.query.scaladsl.ReadJournal和akka.persistence.query.javadsl.ReadJournal的实例。该插件必须同时实现scaladsl和javadsl接口，因为akka.stream.scaladsl.Source和akka.stream.javadsl.Source是不同的类型，即使这些类型可以相互转换，对于最终用户来说也最方便直接访问Java或Scala源。如下所示，一个实现可以委托给另一个实现。
下面是一个简单的日志实现：
```java
static class MyReadJournalProvider implements ReadJournalProvider {
 private final MyJavadslReadJournal javadslReadJournal;
 public MyReadJournalProvider(ExtendedActorSystem system, Config config) { this.javadslReadJournal = new MyJavadslReadJournal(system, config); }
 @Override public MyScaladslReadJournal scaladslReadJournal() { return new MyScaladslReadJournal(javadslReadJournal); }
 @Override public MyJavadslReadJournal javadslReadJournal() { return this.javadslReadJournal; }}
static class MyJavadslReadJournal
 implements akka.persistence.query.javadsl.ReadJournal, akka.persistence.query.javadsl.EventsByTagQuery, akka.persistence.query.javadsl.EventsByPersistenceIdQuery, akka.persistence.query.javadsl.PersistenceIdsQuery, akka.persistence.query.javadsl.CurrentPersistenceIdsQuery {
 private final Duration refreshInterval; private Connection conn;
 public MyJavadslReadJournal(ExtendedActorSystem system, Config config) { refreshInterval = config.getDuration("refresh-interval"); }
 /** * You can use `NoOffset` to retrieve all events with a given tag or retrieve a subset of all * events by specifying a `Sequence` `offset`. The `offset` corresponds to an ordered sequence * number for the specific tag. Note that the corresponding offset of each event is provided in * the [[akka.persistence.query.EventEnvelope]], which makes it possible to resume the stream at * a later point from a given offset. * * <p>The `offset` is exclusive, i.e. the event with the exact same sequence number will not be * included in the returned stream. This means that you can use the offset that is returned in * `EventEnvelope` as the `offset` parameter in a subsequent query. */ @Override public Source<EventEnvelope, NotUsed> eventsByTag(String tag, Offset offset) { if (offset instanceof Sequence) { Sequence sequenceOffset = (Sequence) offset; return Source.fromGraph( new MyEventsByTagSource(conn, tag, sequenceOffset.value(), refreshInterval)); } else if (offset == NoOffset.getInstance()) return eventsByTag(tag, Offset.sequence(0L)); // recursive else throw new IllegalArgumentException( "MyJavadslReadJournal does not support " + offset.getClass().getName() + " offsets"); }
 @Override public Source<EventEnvelope, NotUsed> eventsByPersistenceId( String persistenceId, long fromSequenceNr, long toSequenceNr) { // implement in a similar way as eventsByTag throw new UnsupportedOperationException("Not implemented yet"); }
 @Override public Source<String, NotUsed> persistenceIds() { // implement in a similar way as eventsByTag throw new UnsupportedOperationException("Not implemented yet"); }
 @Override public Source<String, NotUsed> currentPersistenceIds() { // implement in a similar way as eventsByTag throw new UnsupportedOperationException("Not implemented yet"); }
 // possibility to add more plugin specific queries
 public Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) { // implement in a similar way as eventsByTag throw new UnsupportedOperationException("Not implemented yet"); }}
static class MyScaladslReadJournal
 implements akka.persistence.query.scaladsl.ReadJournal, akka.persistence.query.scaladsl.EventsByTagQuery, akka.persistence.query.scaladsl.EventsByPersistenceIdQuery, akka.persistence.query.scaladsl.PersistenceIdsQuery, akka.persistence.query.scaladsl.CurrentPersistenceIdsQuery {
 private final MyJavadslReadJournal javadslReadJournal;
 public MyScaladslReadJournal(MyJavadslReadJournal javadslReadJournal) { this.javadslReadJournal = javadslReadJournal; }
 @Override public akka.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByTag( String tag, akka.persistence.query.Offset offset) { return javadslReadJournal.eventsByTag(tag, offset).asScala(); }
 @Override public akka.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByPersistenceId( String persistenceId, long fromSequenceNr, long toSequenceNr) { return javadslReadJournal .eventsByPersistenceId(persistenceId, fromSequenceNr, toSequenceNr) .asScala(); }
 @Override public akka.stream.scaladsl.Source<String, NotUsed> persistenceIds() { return javadslReadJournal.persistenceIds().asScala(); }
 @Override public akka.stream.scaladsl.Source<String, NotUsed> currentPersistenceIds() { return javadslReadJournal.currentPersistenceIds().asScala(); }
 // possibility to add more plugin specific queries
 public akka.stream.scaladsl.Source<RichEvent, QueryMetadata> byTagsWithMeta( scala.collection.Set<String> tags) { Set<String> jTags = scala.collection.JavaConverters.setAsJavaSetConverter(tags).asJava(); return javadslReadJournal.byTagsWithMeta(jTags).asScala(); }}
```
并且eventsByTag可以由GraphStage支持，例如：
```java
public class MyEventsByTagSource extends GraphStage<SourceShape<EventEnvelope>> {
 public Outlet<EventEnvelope> out = Outlet.create("MyEventByTagSource.out"); private static final String QUERY = "SELECT id, persistence_id, seq_nr, serializer_id, serializer_manifest, payload " + "FROM journal WHERE tag = ? AND id > ? " + "ORDER BY id LIMIT ?";
 enum Continue { INSTANCE; }
 private static final int LIMIT = 1000; private final Connection connection; private final String tag; private final long initialOffset; private final Duration refreshInterval;
 // assumes a shared connection, could also be a factory for creating connections/pool public MyEventsByTagSource( Connection connection, String tag, long initialOffset, Duration refreshInterval) { this.connection = connection; this.tag = tag; this.initialOffset = initialOffset; this.refreshInterval = refreshInterval; }
 @Override public Attributes initialAttributes() { return Attributes.apply(ActorAttributes.IODispatcher()); }
 @Override public SourceShape<EventEnvelope> shape() { return SourceShape.of(out); }
 @Override public GraphStageLogic createLogic(Attributes inheritedAttributes) { return new TimerGraphStageLogic(shape()) { private ActorSystem system = materializer().system(); private long currentOffset = initialOffset; private List<EventEnvelope> buf = new LinkedList<>(); private final Serialization serialization = SerializationExtension.get(system);
 @Override public void preStart() { schedulePeriodically(Continue.INSTANCE, refreshInterval); }
 @Override public void onTimer(Object timerKey) { query(); deliver(); }
 private void deliver() { if (isAvailable(out) && !buf.isEmpty()) { push(out, buf.remove(0)); } }
 private void query() { if (buf.isEmpty()) {
 try (PreparedStatement s = connection.prepareStatement(QUERY)) { s.setString(1, tag); s.setLong(2, currentOffset); s.setLong(3, LIMIT); try (ResultSet rs = s.executeQuery()) { final List<EventEnvelope> res = new ArrayList<>(LIMIT); while (rs.next()) { Object deserialized = serialization .deserialize( rs.getBytes("payload"), rs.getInt("serializer_id"), rs.getString("serializer_manifest")) .get(); currentOffset = rs.getLong("id"); res.add( new EventEnvelope( Offset.sequence(currentOffset), rs.getString("persistence_id"), rs.getLong("seq_nr"), deserialized)); } buf = res; } } catch (Exception e) { failStage(e); } } }
 { setHandler( out, new AbstractOutHandler() { @Override public void onPull() { query(); deliver(); } }); } }; }}
```
ReadJournalProvider类必须具有带有以下签名之一的构造函数：
- 构造函数，带有ExtendedActorSystem参数，com.typesafe.config.Config参数和用于配置路径的String参数
- 构造函数，带有ExtendedActorSystem参数和com.typesafe.config.Config参数
- 具有一个ExtendedActorSystem参数的构造函数
- 没有参数的构造函数
actor系统配置的插件部分将在config构造函数参数中传递。插件的配置路径在String参数中传递。
如果基础数据存储仅支持在到达"结果集"末尾时完成的查询，则日志必须过一会儿才能提交新查询，以支持"无限"事件流，其中包括在初始查询完成后存储的事件完成。建议插件使用名为refresh-interval的配置属性来定义这种刷新间隔。
# 横向扩展
在事件数量非常高的用例中，每个事件需要的工作量很大，或者在弹性方面很重要，因此，如果节点崩溃，则持久性查询会在新节点上快速启动，并可以与群集一起恢复操作事件标记非常适合在群集上分片事件。
建立在Akka之上的Lagom框架对此进行了许多最佳实践的编码。有关更多详细信息，请参阅Lagom文档中的"管理数据持久性和持久性实体"。
## 插件TCK
TODO，尚不可用。
# 示例项目
CQRS示例项目是一个示例项目，可以下载并带有如何运行的说明。
该项目包含一个购物车示例，该示例说明了如何使用Akka Persistence。将事件标记为甚至由处理器使用，以根据事件构建其他表示，或将事件发布到其他服务。